{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Initializing All Required Linbraried & Packages"
      ],
      "metadata": {
        "id": "oWSNj5Hs4CRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfpEcV9RmY_g"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from praw.models import MoreComments\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import csv\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Authentication to handshake with the web sacrpping tool of praw"
      ],
      "metadata": {
        "id": "Rc0_l7z-4IxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install praw\n",
        "\n",
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(client_id='8qmW0-R20V0pIz96MMWvwQ',\n",
        "                     client_secret='gPrXBbyxl0j7MfzsRoi1fWXR43WoAA',\n",
        "                     username='charming_chandler',\n",
        "                     password='sVF^Xfptdu5eq8m',\n",
        "                     user_agent='myproject/0.0.1',\n",
        "                     check_for_async=False)\n"
      ],
      "metadata": {
        "id": "YQLyKDbDp3Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('max_colwidth',None)\n",
        "\n",
        "# subs = reddit.subreddit('funny').top(limit=1)\n",
        "# df =[]\n",
        "\n",
        "# for post in subs:\n",
        "#   url = post.url\n",
        "\n",
        "# # Creating a submission object\n",
        "# submission = reddit.submission(url=url)"
      ],
      "metadata": {
        "id": "yexDpSTbqpyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By giving name of subreddit we are requesting link of top post in the given subreddit"
      ],
      "metadata": {
        "id": "oxyhznor4Xjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total No. Of Post In Subreddits Limit\n",
        "#step 1\n",
        "sub_name = 'funny'\n",
        "subs = reddit.subreddit(sub_name).top(limit=5)\n",
        "for post in subs:\n",
        "  url = \"https://www.reddit.com\" + post.permalink\n",
        "  print(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af7EXjM2qvyS",
        "outputId": "aee1cd8a-ab52-45b4-9c23-10ee7a3d92b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.reddit.com/r/funny/comments/7mjw12/my_cab_driver_tonight_was_so_excited_to_share/\n",
            "https://www.reddit.com/r/funny/comments/5gn8ru/guardians_of_the_front_page/\n",
            "https://www.reddit.com/r/funny/comments/7431qq/gas_station_worker_takes_precautionary_measures/\n",
            "https://www.reddit.com/r/funny/comments/7kvjuz/the_conversation_my_son_and_i_will_have_on/\n",
            "https://www.reddit.com/r/funny/comments/j0w79j/the_denver_broncos_have_the_entire_town_of_south/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By giving the name of User checking that how many post have been done by the user in given subreddit."
      ],
      "metadata": {
        "id": "h6frYGxF45V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In The Subredit Given User Is Holding Post's Or Not if yes then printing how many?\n",
        "#step 2\n",
        "cnt=0\n",
        "user_name = 'iH8myPP'\n",
        "subreddit = reddit.subreddit(sub_name)\n",
        "user = reddit.redditor(user_name)\n",
        "# Get the posts from the user in the subreddit\n",
        "user_posts = []\n",
        "for submission in user.submissions.top(limit=None):\n",
        "    if submission.subreddit == subreddit:\n",
        "        user_posts.append(submission)\n",
        "\n",
        "# Print the titles of the user's posts in the subreddit\n",
        "for post in user_posts:\n",
        "    cnt += 1\n",
        "print(cnt) #number of posts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2hOw0b6r8Ei",
        "outputId": "1f52bb9d-2f56-4242-ecee-82f62d274e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching intial/preffered dataset i.e username and total no. of comments that user got in across-post & in-post"
      ],
      "metadata": {
        "id": "0WctBXft5Ebk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step 3\n",
        "from praw.models import MoreComments\n",
        "globalDict = {}\n",
        "dict_df = []\n",
        "for post in user_posts:\n",
        "  temp = \"https://www.reddit.com\" + post.permalink\n",
        "  submission = reddit.submission(url=temp)\n",
        "  myList = []\n",
        "  # mySet = set(myList)\n",
        "  \n",
        "  for top_level_comment in submission.comments:\n",
        "    if isinstance(top_level_comment, MoreComments):\n",
        "        continue\n",
        "    #print(top_level_comment.author)\n",
        "    # mySet.add(top_level_comment.author)\n",
        "    if(top_level_comment.author not in globalDict):\n",
        "      globalDict[top_level_comment.author] = 1\n",
        "    else:\n",
        "      globalDict[top_level_comment.author] += 1\n",
        "  # dict_df.append([cnt,myList])\n",
        "  # globalDict[cnt] = mySet\n",
        "  # for s in mySet:\n",
        "  #   if(s not in globalDict):\n",
        "  #     globalDict[s] = 1\n",
        "  #   else:\n",
        "  #     globalDict[s] += 1\n",
        "dict_df = pd.DataFrame.from_dict(globalDict, orient = 'index',columns = ['count'])\n",
        "dict_df = dict_df.sort_values(by = 'count', ascending = False)\n",
        "dict_df"
      ],
      "metadata": {
        "id": "ruQMmGmCsPrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on name of total users we are making final Dataset Sheet of username vs total no. of comments user got."
      ],
      "metadata": {
        "id": "5zFqbNet5bZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing name of all users of subredits and we are getting total no of interaction(Comments) user wise. \n",
        "#step 3\n",
        "from praw.models import MoreComments\n",
        "globalDict = {'iH8myPP','sellyourcomputer','txhorns1330','SrGrafo','JediWithAnM4',\n",
        "              'socialpronk','sirmakoto','ReallyRickAstley','DaFunkJunkie','stem12345679',\n",
        "              'DoremusJessup','pipsdontsqueak','chrisdh79','Science_News','ScienceModerator',\n",
        "              'Meltingteeth','Miskatonica'}\n",
        "data = []\n",
        "# Create a CSV file\n",
        "with open('hehe.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    writer.writerow(['number of interaction', 'source'])\n",
        "    globalDict = {'calibrono'}\n",
        "    dict_df = []\n",
        "    for user in globalDict:\n",
        "      username = reddit.redditor(user)\n",
        "      posts = username.submissions.top(limit=5)\n",
        "      cnt = 0\n",
        "      for post in posts:\n",
        "        temp = \"https://www.reddit.com\" + post.permalink\n",
        "        submission = reddit.submission(url=temp)\n",
        "        # print(submission.num_comments)\n",
        "        num = submission.num_comments\n",
        "        cnt = cnt + submission.num_comments\n",
        "      writer.writerow([cnt,user])"
      ],
      "metadata": {
        "id": "-mx5DP1vvYPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "G7y3bYoawExV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE VERSION_2"
      ],
      "metadata": {
        "id": "6ACU-rN9vQL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding top 50 posts of the given subreddit"
      ],
      "metadata": {
        "id": "gnu8MydR6KoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step 1 - step one to get the all the username from the top50 posts of the specific subreddit\n",
        "sub_name = 'funny' # name of the subreddit\n",
        "authors = []\n",
        "checking = []\n",
        "\n",
        "subs = reddit.subreddit(sub_name).top(limit=50)\n",
        "\n",
        "for post in subs:\n",
        "  url = \"https://www.reddit.com\" + post.permalink\n",
        "  temp = url\n",
        "  print(temp)\n",
        "  submission = reddit.submission(url=temp)\n",
        "  try:\n",
        "      if submission.author.is_suspended:\n",
        "        print(\"Author attribute not found, skipping.\")\n",
        "  except AttributeError:\n",
        "      if submission.author is not None:\n",
        "        # countsus = countsus + 1\n",
        "        authors.append(submission.author.name)\n",
        "\n",
        "# print('countsus: ', countsus)\n",
        "\n",
        "# print(authors)\n",
        "\n",
        "uniqueUserList = []\n",
        "[uniqueUserList.append(x) for x in authors if x not in uniqueUserList]\n"
      ],
      "metadata": {
        "id": "Euefjqd3xy7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using subredit name we are getting dataset of top 50 post by giving name of users that have made post in the subreddits"
      ],
      "metadata": {
        "id": "RMW5qoWN6ksB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting csv file based on name of subreddit \n",
        "#step 2 \n",
        "from praw.models import MoreComments\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# globalDict = {}\n",
        "dict_df = []\n",
        "ans = []\n",
        "cnt=0\n",
        "# userList = ['iH8myPP']\n",
        "subreddit = reddit.subreddit(sub_name)\n",
        "# Get the posts from the user in the subreddit\n",
        "user_posts = []\n",
        "# Open the CSV file for writing\n",
        "with open(sub_name+\"_json.csv\", \"w\", newline=\"\") as csvfile:\n",
        "  # Create a CSV writer object\n",
        "  writer = csv.writer(csvfile)\n",
        "  # Write the header row\n",
        "  writer.writerow([\"Source\",\"Target\", \"Weight\"])\n",
        "  for username in uniqueUserList:\n",
        "    ans.clear()\n",
        "    globalDict = {}\n",
        "    user_posts = []\n",
        "    user = reddit.redditor(username)\n",
        "    for submission in user.submissions.top(limit=None):\n",
        "        if submission.subreddit == subreddit:\n",
        "            user_posts.append(submission)\n",
        "\n",
        "    for post in user_posts:\n",
        "      temp = \"https://www.reddit.com\" + post.permalink\n",
        "      submission = reddit.submission(url=temp)\n",
        "      myList = []\n",
        "      # mySet = set(myList)\n",
        "      \n",
        "      for top_level_comment in submission.comments:\n",
        "        if isinstance(top_level_comment, MoreComments):\n",
        "            continue\n",
        "        if(top_level_comment.author not in globalDict):\n",
        "          globalDict[top_level_comment.author] = 1\n",
        "        else:\n",
        "          globalDict[top_level_comment.author] += 1\n",
        "          if(globalDict[top_level_comment.author] > 1 and top_level_comment.author is not None and\n",
        "             top_level_comment.author != username and top_level_comment.author != \"AutoModerator\"\n",
        "              and top_level_comment.author != \"autotldr\"):\n",
        "            cmnt = top_level_comment.author\n",
        "            # print(\"cmnt is: \",cmnt)\n",
        "            ans.append(cmnt)\n",
        "    res = []\n",
        "    [res.append(x) for x in ans if x not in res]\n",
        "\n",
        "    for usern in res:\n",
        "      if(usern is not None):\n",
        "        dict_df.append([usern,user,globalDict[usern]])\n",
        "        writer.writerow([usern,user,globalDict[usern]])\n",
        "        # print(\"usern is: \",usern)\n",
        "    ans.clear()\n",
        "    # print(\"after clearing the ans: \",ans)\n",
        "    # print(\"user done\")\n",
        "\n",
        "  dict_df = pd.DataFrame(dict_df,columns=[\"Source\",\"Target\", \"Weight\"])\n",
        "  dict_df = dict_df.sort_values(by=\"Weight\", ascending=False)\n",
        "  dict_df\n",
        "  dict_df.to_csv(sub_name+\"_df.csv\", index = False)"
      ],
      "metadata": {
        "id": "RrLROvUUx3cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second part of code Fetching"
      ],
      "metadata": {
        "id": "_XPxIKhR7Msa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################\n",
        "#################################################\n",
        "#################################################\n",
        "##################FETCHING DATA##################\n",
        "#################################################\n",
        "#################################################\n",
        "#################################################"
      ],
      "metadata": {
        "id": "-4Nkgvc32-Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Degree centrality of the network"
      ],
      "metadata": {
        "id": "qZD3E2543a21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import csv\n",
        "\n",
        "# Load data from Excel file\n",
        "df = pd.read_excel('SNA_All_Nodes.xlsx')\n",
        "\n",
        "# Create a directed graph using NetworkX\n",
        "# Add edges from the DataFrame\n",
        "\n",
        "G = nx.from_pandas_edgelist(df, source='Source', target='Target', edge_attr='Weight', create_using=nx.DiGraph())\n",
        "\n",
        "# Calculate degree centrality\n",
        "in_degree_centrality = nx.in_degree_centrality(G)\n",
        "\n",
        "\n",
        "# Print all entries in sorted order\n",
        "sorted_nodes = sorted(in_degree_centrality, key=in_degree_centrality.get, reverse=True)\n",
        "#print(len(sorted_centrality))\n",
        "# Print the top 5 entries\n",
        "# write nodes and in-degree centrality to a CSV file\n",
        "with open('in_degree_centrality.csv', mode='w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Node', 'In-degree centrality'])\n",
        "    for node in sorted_nodes:\n",
        "        writer.writerow([node, str(in_degree_centrality[node])])"
      ],
      "metadata": {
        "id": "JqQiLTZE3NV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvector Centrality of the network"
      ],
      "metadata": {
        "id": "481Cr6pn3eUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_user = []\n",
        "\n",
        "# calculate the eigenvector centrality using numpy\n",
        "eigen_centrality = nx.eigenvector_centrality_numpy(G)\n",
        "\n",
        "sorted_nodes = sorted(eigen_centrality, key=eigen_centrality.get, reverse=True)\n",
        "\n",
        "with open('eigen_centrality_1.csv', mode='w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Node', 'eigen_centrality'])\n",
        "    for node in sorted_nodes:\n",
        "        writer.writerow([node, str(eigen_centrality[node])])"
      ],
      "metadata": {
        "id": "8Ts0ASaz3e5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong links and weak links of different subreddits"
      ],
      "metadata": {
        "id": "yFtnBj913gsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load edge list from CSV file\n",
        "df = pd.read_excel('tes.xlsx')\n",
        "\n",
        "# Create weighted graph from edge list\n",
        "G = nx.from_pandas_edgelist(df, source='Source', target='Target', edge_attr='Weight', create_using=nx.DiGraph())\n",
        "\n",
        "# Calculate weighted degree centrality\n",
        "degree_dict = dict(G.degree(weight='Weight'))\n",
        "\n",
        "# Define color map based on node weights\n",
        "color_map = []\n",
        "for node in G.nodes():\n",
        "    if degree_dict[node] >= 3:\n",
        "        color_map.append('green')\n",
        "    else:\n",
        "        color_map.append('blue')\n",
        "\n",
        "# Draw graph with node colors\n",
        "fig, ax = plt.subplots(figsize=(15, 20))\n",
        "nx.draw(G, node_color=color_map)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tG9w7M_L3jd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Community detection in different subreddits"
      ],
      "metadata": {
        "id": "979whbtP3l6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from networkx.algorithms.community import asyn_lpa_communities\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data from Excel file\n",
        "df = pd.read_excel('SNA_All_Nodes.xlsx')\n",
        "\n",
        "# Create a directed graph using NetworkX\n",
        "G = nx.from_pandas_edgelist(df, source='Source', target='Target', edge_attr='Weight', create_using=nx.DiGraph())\n",
        "\n",
        "# Use asyn_lpa_communities() for community detection\n",
        "communities_generator = asyn_lpa_communities(G, weight='Weight')\n",
        "communities = [list(c) for c in communities_generator]\n",
        "\n",
        "# Create a dictionary of colors for each community\n",
        "color_map = {}\n",
        "for i, community in enumerate(communities):\n",
        "    for node in community:\n",
        "        color_map[node] = i\n",
        "\n",
        "# Plot the graph with nodes colored by community\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(1,figsize=(20,20))\n",
        "nx.draw_networkx_nodes(G, pos, node_color=[color_map[node] for node in G.nodes()], cmap=plt.cm.tab20)\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
        "# nx.draw_networkx_labels(G, pos)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aeBcYdxw3pFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "log-log plots of degree distribution for different subreddits (power law verification)"
      ],
      "metadata": {
        "id": "_P9EjaUm3rDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "\n",
        "sub_reddi = ['funny','AskReddit','gaming','aww','music','pics','worldNews','Science','videos','todayilearned','movies','news','EarthPorn','gifs','IAmA','food','askscience','jokes','explainlikeiamfive','art']\n",
        "\n",
        "for temp in sub_reddi:\n",
        "  # Load data from Excel file\n",
        "  df = pd.read_excel(temp+'.xlsx')\n",
        "\n",
        "  G = nx.from_pandas_edgelist(df, source='Source', target='Target', edge_attr='Weight', create_using=nx.DiGraph())\n",
        "\n",
        "  # Compute in-degree and out-degree of each node\n",
        "  in_degrees = dict(G.in_degree())\n",
        "  out_degrees = dict(G.out_degree())\n",
        "\n",
        "  # Plot the degree distributions on a log-log scale\n",
        "  in_degree_sequence = sorted([d for n, d in in_degrees.items()], reverse=True)\n",
        "  out_degree_sequence = sorted([d for n, d in out_degrees.items()], reverse=True)\n",
        "\n",
        "  plt.loglog(in_degree_sequence, 'b-', marker='o')\n",
        "  plt.loglog(out_degree_sequence, 'r-', marker='o')\n",
        "  plt.legend(['In-degree', 'Out-degree'])\n",
        "  plt.xlabel('Degree')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.savefig(temp, bbox_inches='tight')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "XHzqqVbZ3tmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Degree distributions of different subreddits"
      ],
      "metadata": {
        "id": "xh4Cz5ZD3v47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read the CSV file into a Pandas dataframe\n",
        "df = pd.read_csv('music.csv')\n",
        "\n",
        "# create a directed graph using NetworkX\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# add edges with weights from the dataframe to the graph\n",
        "for row in df.itertuples(index=False):\n",
        "    G.add_edge(row.Source, row.Target, weight=row.Weight)\n",
        "\n",
        "# calculate the in-degree and out-degree distributions\n",
        "in_degrees = [d for n, d in G.in_degree()]\n",
        "out_degrees = [d for n, d in G.out_degree()]\n",
        "\n",
        "print(\"Music\")\n",
        "print(\"In-degree distribution\")\n",
        "print(in_degrees)\n",
        "print(\"Out-degree distribution\")\n",
        "print(out_degrees)\n",
        "\n",
        "# plot the histograms\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,15))\n",
        "ax[0].hist(in_degrees, bins=20)\n",
        "ax[0].set_title('In-degree distribution')\n",
        "ax[0].set_xlabel('Degree')\n",
        "ax[0].set_ylabel('Number of nodes')\n",
        "ax[1].hist(out_degrees, bins=20)\n",
        "ax[1].set_title('Out-degree distribution')\n",
        "ax[1].set_xlabel('Degree')\n",
        "ax[1].set_ylabel('Number of nodes')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HVidquaX30Bu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}